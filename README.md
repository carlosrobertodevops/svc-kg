
# svc-kg

svc-kg/
â”œâ”€ app.py
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.local.yml
â”œâ”€ docker-compose.coolify.yml
â”œâ”€ .env.example
â”œâ”€ .gitignore
â”œâ”€ .dockerignore
â”œâ”€ README.md
â”œâ”€ assets/           # (montado no container)
â”‚  â””â”€ .keep
â”œâ”€ static/           # (montado no container)
â”‚  â””â”€ .keep
â”œâ”€ tmp/              # (montado no container)
â”‚  â””â”€ .gitkeep
â”œâ”€ docs/
â”‚  â””â”€ openapi.yaml   # Swagger spec estÃ¡tico (usado no /docs)
â””â”€ db/
   â”œâ”€ 00_init.sql    # schema + seed + get_graph_membros
   â”œâ”€ 01_indexes.sql # Ã­ndices
   â””â”€ 02_alias.sql   # et_graph_membros -> get_graph_membros





Micro-serviÃ§o FastAPI para exibir grafos (pyVis) a partir de RPC no Supabase.



## Endpoints

- `GET /health` â†’ `?deep=1` para verificar conexÃ£o com Supabase
- `GET /graph/membros` â†’ JSON (nodes/edges) com meta
- `GET /graph/membros/vis` â†’ HTML pyVis
- `POST /rpc/get_graph_membros` â†’ debug pass-through

### ParÃ¢metros comuns
- `p_faccao_id`: string
- `p_include_co`: bool
- `p_max_pairs`: int
- `depth`: int
- `preview`: bool (default: true) â€“ aplica truncamento
- `max_nodes`: int (default: 500)
- `max_edges`: int (default: 1000)
- `nocache`: bool â€“ ignora cache do servidor
- `cache_ttl`: int â€“ TTL customizado (segundos)







## Exemplo cURL

---
```bash
curl "https://svc-kg.SEUDOMINIO/graph/membros?p_faccao_id=abc123&preview=true"
curl "https://svc-kg.SEUDOMINIO/graph/membros/vis?p_faccao_id=abc123&physics=true"
curl -X POST "https://svc-kg.SEUDOMINIO/rpc/get_graph_membros" \
  -H "Content-Type: application/json" \
  -d '{"p_faccao_id":"abc123","p_include_co":true,"p_max_pairs":200}'

# DocumentaÃ§Ã£o

- Swagger UI: `GET /docs`
- Redoc: `GET /redoc`
- OpenAPI JSON gerado: `GET /openapi.json`
- OpenAPI YAML estÃ¡tico: `GET /openapi.yaml`

Endpoints principais:
- `GET /graph/members` â†’ JSON (nodes/edges) para FlutterFlow
- `GET /graph/members/html` â†’ PÃ¡gina HTML pyVis pronta para embed
- `GET /health` â†’ status

```
---


---

## ðŸ”Ž Checklist agora (externo)

1. Abra no browser:  
   `https://svc-kg.mondaha.com/live`  
   Esperado: `200 {"status":"live",...}`

2. Se (1) estiver ok, teste:  
   `https://svc-kg.mondaha.com/health`  
   Deve vir `200`.

3. Depois:  
   `https://svc-kg.mondaha.com/ready`  
   - `200`: tudo certo (Redis/backend ok).  
   - `503`: a resposta JSON aponta o que estÃ¡ falhando (ver campo `error`).

4. Por fim, a rota do grafo:  
   `https://svc-kg.mondaha.com/v1/graph/membros?faccao_id=6&include_co=true&max_pairs=500`

Se **(1)** ainda der **503**, o problema Ã© 100% de **roteamento/porta no Coolify** (o Traefik nÃ£o encontra container saudÃ¡vel na porta interna). A correÃ§Ã£o Ã© alinhar **PORT** + **Application Port** + (opcional) **labels** acima.
::contentReference[oaicite:0]{index=0}



## Troubleshooting 503 ("no available server")

Esse 503 vem do Traefik/edge. Siga:

1) **App port interno**
   - No Coolify, verifique **Application Port** do serviÃ§o. Deve ser **8080** (ou altere `PORT` nas envs do app para o valor da UI).
   - Nosso container escuta em `0.0.0.0:${PORT:-8080}`.

2) **Healthcheck no Coolify**
   - Use `/live` (nÃ£o depende de Redis/DB).
   - Se o health estiver falhando, o Traefik nÃ£o publica o serviÃ§o.

3) **Teste direto do app (no host do container)**
   ```bash
   docker exec -it <container-svc-kg> sh -lc 'curl -sS http://localhost:${PORT:-8080}/live && echo'
   docker exec -it <container-svc-kg> sh -lc 'curl -sS http://localhost:${PORT:-8080}/ready && echo'

