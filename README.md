
# svc-kg

svc-kg/
‚îú‚îÄ app.py
‚îú‚îÄ Dockerfile
‚îú‚îÄ docker-compose.local.yml
‚îú‚îÄ docker-compose.coolify.yml
‚îú‚îÄ .env.example
‚îú‚îÄ .gitignore
‚îú‚îÄ .dockerignore
‚îú‚îÄ README.md
‚îú‚îÄ assets/           # (montado no container)
‚îÇ  ‚îî‚îÄ .keep
‚îú‚îÄ static/           # (montado no container)
‚îÇ  ‚îî‚îÄ .keep
‚îú‚îÄ tmp/              # (montado no container)
‚îÇ  ‚îî‚îÄ .gitkeep
‚îú‚îÄ docs/
‚îÇ  ‚îî‚îÄ openapi.yaml   # Swagger spec est√°tico (usado no /docs)
‚îî‚îÄ db/
   ‚îú‚îÄ 00_init.sql    # schema + seed + get_graph_membros
   ‚îú‚îÄ 01_indexes.sql # √≠ndices
   ‚îî‚îÄ 02_alias.sql   # et_graph_membros -> get_graph_membros





# svc-kg

Microservi√ßo de Knowledge Graph com:
- Backend: Supabase RPC (`get_graph_membros`) ou Postgres direto.
- Cache: Redis (fallback mem√≥ria).
- Visual: endpoint **/v1/vis/pyvis** (HTML interativo PyVis).

## Subir LOCAL
```bash
cp .env.example .env
# Deixe SUPABASE_* em branco e use o Postgres local do compose
docker compose -f docker-compose.local.yml up --build
curl -s http://localhost:8080/ready | jq


Micro-servi√ßo FastAPI para exibir grafos (pyVis) a partir de RPC no Supabase.



## Endpoints

- `GET /health` ‚Üí `?deep=1` para verificar conex√£o com Supabase
- `GET /graph/membros` ‚Üí JSON (nodes/edges) com meta
- `GET /graph/membros/vis` ‚Üí HTML pyVis
- `POST /rpc/get_graph_membros` ‚Üí debug pass-through

### Par√¢metros comuns
- `p_faccao_id`: string
- `p_include_co`: bool
- `p_max_pairs`: int
- `depth`: int
- `preview`: bool (default: true) ‚Äì aplica truncamento
- `max_nodes`: int (default: 500)
- `max_edges`: int (default: 1000)
- `nocache`: bool ‚Äì ignora cache do servidor
- `cache_ttl`: int ‚Äì TTL customizado (segundos)







## Exemplo cURL

---
```bash
curl "https://svc-kg.SEUDOMINIO/graph/membros?p_faccao_id=abc123&preview=true"
curl "https://svc-kg.SEUDOMINIO/graph/membros/vis?p_faccao_id=abc123&physics=true"
curl -X POST "https://svc-kg.SEUDOMINIO/rpc/get_graph_membros" \
  -H "Content-Type: application/json" \
  -d '{"p_faccao_id":"abc123","p_include_co":true,"p_max_pairs":200}'

# Documenta√ß√£o

- Swagger UI: `GET /docs`
- Redoc: `GET /redoc`
- OpenAPI JSON gerado: `GET /openapi.json`
- OpenAPI YAML est√°tico: `GET /openapi.yaml`

Endpoints principais:
- `GET /graph/members` ‚Üí JSON (nodes/edges) para FlutterFlow
- `GET /graph/members/html` ‚Üí P√°gina HTML pyVis pronta para embed
- `GET /health` ‚Üí status

```
---


---

## üîé Checklist agora (externo)

1. Abra no browser:  
   `https://svc-kg.mondaha.com/live`  
   Esperado: `200 {"status":"live",...}`

2. Se (1) estiver ok, teste:  
   `https://svc-kg.mondaha.com/health`  
   Deve vir `200`.

3. Depois:  
   `https://svc-kg.mondaha.com/ready`  
   - `200`: tudo certo (Redis/backend ok).  
   - `503`: a resposta JSON aponta o que est√° falhando (ver campo `error`).

4. Por fim, a rota do grafo:  
   `https://svc-kg.mondaha.com/v1/graph/membros?faccao_id=6&include_co=true&max_pairs=500`

Se **(1)** ainda der **503**, o problema √© 100% de **roteamento/porta no Coolify** (o Traefik n√£o encontra container saud√°vel na porta interna). A corre√ß√£o √© alinhar **PORT** + **Application Port** + (opcional) **labels** acima.
::contentReference[oaicite:0]{index=0}



## Troubleshooting 503 ("no available server")

Esse 503 vem do Traefik/edge. Siga:

1) **App port interno**
   - No Coolify, verifique **Application Port** do servi√ßo. Deve ser **8080** (ou altere `PORT` nas envs do app para o valor da UI).
   - Nosso container escuta em `0.0.0.0:${PORT:-8080}`.

2) **Healthcheck no Coolify**
   - Use `/live` (n√£o depende de Redis/DB).
   - Se o health estiver falhando, o Traefik n√£o publica o servi√ßo.

3) **Teste direto do app (no host do container)**
   ```bash
   docker exec -it <container-svc-kg> sh -lc 'curl -sS http://localhost:${PORT:-8080}/live && echo'
   docker exec -it <container-svc-kg> sh -lc 'curl -sS http://localhost:${PORT:-8080}/ready && echo'

